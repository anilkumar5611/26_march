{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b38042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Simple Linear Regression vs. Multiple Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8eb3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression: In simple linear regression, there is only one\n",
    "#     independent variable predicting the dependent variable. The relationship \n",
    "#     between the variables is modeled as a straight line.\n",
    "\n",
    "# Example: Predicting the salary of employees based on their years of experience.\n",
    "\n",
    "# Multiple Linear Regression: In multiple linear regression, there are multiple \n",
    "#     independent variables predicting the dependent variable. The relationship \n",
    "#     between the variables is modeled as a linear combination.\n",
    "\n",
    "# Example: Predicting house prices based on factors like square footage, number \n",
    "#     of bedrooms, and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1dc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Assumptions of Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7694516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearity: The relationship between the independent and dependent\n",
    "#     variables should be linear.\n",
    "\n",
    "# Independence: The residuals (errors) should be independent of each other.\n",
    "\n",
    "# Homoscedasticity: The variance of the residuals should be constant across\n",
    "#     all levels of the independent variables.\n",
    "\n",
    "# Normality: The residuals should be normally distributed.\n",
    "\n",
    "# To check these assumptions, you can use techniques like residual plots,\n",
    "# Durbin-Watson test for independence, Breusch-Pagan test for homoscedasticity,\n",
    "# and Shapiro-Wilk test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f73d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Interpretation of Slope and Intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934acb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope: It represents the change in the dependent variable\n",
    "#     for a one-unit change in the independent variable, holding\n",
    "#     all other variables constant.\n",
    "\n",
    "# Intercept: It represents the value of the dependent variable when \n",
    "    \n",
    "#     all independent variables are zero.\n",
    "\n",
    "# Example: In a salary prediction model, the slope indicates how much the\n",
    "#     salary increases for each additional year of experience, while the \n",
    "#     intercept represents the starting salary for someone with zero years \n",
    "#     of experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91cbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facbaf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent is an optimization algorithm used to minimize the\n",
    "# loss function in machine learning models. It iteratively adjusts th\n",
    "# e parameters of the model in the direction of the steepest descent \n",
    "# of the loss function.\n",
    "\n",
    "# It is used in machine learning to find the optimal parameters (weights \n",
    "# and biases) of models, such as linear regression, logistic regression,\n",
    "# and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b88103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Multiple Linear Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7dee646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression is an extension of simple linear \n",
    "# regression where there are multiple independent variables predicting \n",
    "# the dependent variable.\n",
    "\n",
    "# It differs from simple linear regression in that it can capture more \n",
    "# complex relationships between the predictors and the response variable \n",
    "# by considering multiple factors simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e0df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Multicollinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a4615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity occurs when independent variables in a \n",
    "# multiple linear regression model are highly correlated with each other.\n",
    "\n",
    "# To detect multicollinearity, you can calculate the correlation matrix\n",
    "# between independent variables. Techniques to address multicollinearity \n",
    "# include removing one of the correlated variables, combining correlated \n",
    "# variables, or using regularization techniques like Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07eb2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Polynomial Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "481f5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression is a form of regression analysis in which\n",
    "# the relationship between the independent variable and the dependent\n",
    "# variable is modeled as an nth degree polynomial.\n",
    "\n",
    "# It differs from linear regression by allowing the relationship between\n",
    "# the independent and dependent variables to be nonlinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6e6119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Advantages and Disadvantages of Polynomial Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e9233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages:\n",
    "\n",
    "# Can capture nonlinear relationships between variables.\n",
    "# More flexible than linear regression.\n",
    "# Can fit complex data patterns.\n",
    "# Disadvantages:\n",
    "\n",
    "# Susceptible to overfitting, especially with higher degree polynomials.\n",
    "# Interpretation of coefficients becomes more complex.\n",
    "# Extrapolation beyond the range of observed data can be unreliable.\n",
    "# Polynomial regression is preferred when the relationship between variables \n",
    "# is nonlinear and simple linear regression cannot adequately capture it. However\n",
    "# , caution should be exercised to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab443d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
